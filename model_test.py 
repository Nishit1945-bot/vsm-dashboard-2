# Install necessary packages if not already installed
# pip install transformers huggingface_hub torch

from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
import torch

#  Log in to Hugging Face to access private models
login("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")  # only needed once per session

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("nishit1945/VSM-LLM")
model = AutoModelForCausalLM.from_pretrained("nishit1945/VSM-LLM")
s
# Optional: put model on GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

#  Prepare input
prompt = "Hello, can you summarize AI in one sentence?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

#  Generate output
outputs = model.generate(
    **inputs,
    max_new_tokens=100,  # limit output length
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

#  Decode generated text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Model output:", generated_text)
